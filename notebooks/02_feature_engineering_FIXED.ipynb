{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline - FIXED (No Data Leakage)\n",
    "\n",
    "**Problem:** Oryginalny notebook obliczaÅ‚ cechy uÅ¼ywajÄ…c WSZYSTKICH transakcji, wÅ‚Ä…cznie z transakcjÄ… ktÃ³rÄ… prÃ³bujemy przewidzieÄ‡.\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "- **User features:** Obliczamy z POPRZEDNICH transakcji tego uÅ¼ytkownika (cumulative)\n",
    "- **Product/Category features:** Obliczamy globalnie, ale wykluczamy aktualnÄ… transakcjÄ™\n",
    "- **RFM features:** Tylko frequency (bez monetary, ktÃ³ry zawiera Purchase)\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. Dodajemy `transaction_id` jako kolejnoÅ›Ä‡ transakcji dla kaÅ¼dego uÅ¼ytkownika\n",
    "2. UÅ¼ywamy Window Functions z `rowsBetween(unboundedPreceding, -1)` dla user features\n",
    "3. Dla product/category: obliczamy total stats, potem odejmujemy aktualnÄ… transakcjÄ™\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:11:35.243826Z",
     "start_time": "2025-11-30T21:11:35.059823Z"
    }
   },
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as _sum, avg, min as _min, max as _max,\n",
    "    countDistinct, stddev, when, lit, dense_rank, percent_rank,\n",
    "    broadcast, row_number, lag, monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from config.spark_config import SparkConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:11:41.360951Z",
     "start_time": "2025-11-30T21:11:38.348204Z"
    }
   },
   "source": [
    "spark = SparkConfig.get_spark_session(\"BlackFriday-FeatureEngineering-FIXED\")\n",
    "print(\"\\nSpark session ready!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Windows: HADOOP_HOME set to C:\\Users\\usate\\PycharmProjects\\BlackFriday\\hadoop\n",
      "  Note: If you encounter permission errors, download winutils.exe\n",
      "  from https://github.com/steveloughran/winutils and place in hadoop/bin/\n",
      "============================================================\n",
      "Spark Session Created: BlackFriday-FeatureEngineering-FIXED\n",
      "============================================================\n",
      "Spark Version: 3.5.3\n",
      "Master: local[*]\n",
      "App Name: BlackFriday-FeatureEngineering-FIXED\n",
      "============================================================\n",
      "\n",
      "Spark session ready!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:11:46.715106Z",
     "start_time": "2025-11-30T21:11:43.471582Z"
    }
   },
   "source": [
    "# Load raw data\n",
    "df_raw = spark.read.csv(\n",
    "    \"../data/raw/train.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df_raw.count():,} rows x {len(df_raw.columns)} columns\")\n",
    "df_raw.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 550,068 rows x 12 columns\n",
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:11:51.949836Z",
     "start_time": "2025-11-30T21:11:51.005322Z"
    }
   },
   "source": [
    "# Clean data\n",
    "df_clean = df_raw.dropDuplicates()\n",
    "\n",
    "# Fill missing Product_Category_2 and Product_Category_3 with 0\n",
    "df_clean = df_clean.fillna({\n",
    "    'Product_Category_2': 0,\n",
    "    'Product_Category_3': 0\n",
    "})\n",
    "\n",
    "print(f\"Cleaned data: {df_clean.count():,} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data: 550,068 rows\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Transaction Ordering\n",
    "\n",
    "**Problem:** Dataset nie ma timestampÃ³w.\n",
    "\n",
    "**RozwiÄ…zanie:** Dodajemy `transaction_order` dla kaÅ¼dego uÅ¼ytkownika uÅ¼ywajÄ…c `row_number()`.\n",
    "- Sortujemy po User_ID i uÅ¼ywamy row_number() jako proxy dla czasu\n",
    "- Dla kaÅ¼dej transakcji, bÄ™dziemy obliczaÄ‡ cechy z POPRZEDNICH transakcji (lower transaction_order)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:12:11.417763Z",
     "start_time": "2025-11-30T21:12:10.099763Z"
    }
   },
   "source": [
    "# Add transaction order for each user\n",
    "# We'll use row_number() partitioned by User_ID as a proxy for transaction time\n",
    "window_user_order = Window.partitionBy(\"User_ID\").orderBy(monotonically_increasing_id())\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"transaction_order\",\n",
    "    row_number().over(window_user_order)\n",
    ")\n",
    "\n",
    "print(\"Transaction order added\")\n",
    "df_clean.select(\"User_ID\", \"Product_ID\", \"Purchase\", \"transaction_order\").show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction order added\n",
      "+-------+----------+--------+-----------------+\n",
      "|User_ID|Product_ID|Purchase|transaction_order|\n",
      "+-------+----------+--------+-----------------+\n",
      "|1000004| P00025442|   19693|                1|\n",
      "|1000004| P00217442|   15499|                2|\n",
      "|1000004|  P0097242|   15686|                3|\n",
      "|1000004| P00184942|   19215|                4|\n",
      "|1000004| P00329542|    3849|                5|\n",
      "|1000004| P00114942|   19120|                6|\n",
      "|1000004| P00318742|   15853|                7|\n",
      "|1000004| P00375436|     481|                8|\n",
      "|1000004| P00112142|   11765|                9|\n",
      "|1000004| P00346142|   15854|               10|\n",
      "+-------+----------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. User Features (Cumulative - No Leakage)\n",
    "\n",
    "**Key Change:** UÅ¼ywamy `rowsBetween(Window.unboundedPreceding, -1)` aby wykluczyÄ‡ aktualnÄ… transakcjÄ™.\n",
    "\n",
    "Dla kaÅ¼dej transakcji uÅ¼ytkownika, obliczamy:\n",
    "- user_purchase_count: liczba POPRZEDNICH transakcji\n",
    "- user_avg_purchase: Å›rednia z POPRZEDNICH transakcji\n",
    "- user_total_spent: suma POPRZEDNICH transakcji\n",
    "- etc.\n",
    "\n",
    "**Uwaga:** Pierwsza transakcja uÅ¼ytkownika bÄ™dzie miaÅ‚a NULL dla tych cech (bo nie ma poprzednich)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:12:36.259930Z",
     "start_time": "2025-11-30T21:12:34.805658Z"
    }
   },
   "source": [
    "# Define window for cumulative user features\n",
    "# Key: rowsBetween(unboundedPreceding, -1) excludes current row!\n",
    "window_user = Window.partitionBy(\"User_ID\").orderBy(\"transaction_order\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# User aggregations (cumulative, excluding current transaction)\n",
    "df_enriched = df_clean.withColumn(\n",
    "    \"user_purchase_count_prior\",\n",
    "    count(\"Purchase\").over(window_user)\n",
    ").withColumn(\n",
    "    \"user_total_spent_prior\",\n",
    "    _sum(\"Purchase\").over(window_user)\n",
    ").withColumn(\n",
    "    \"user_avg_purchase_prior\",\n",
    "    avg(\"Purchase\").over(window_user)\n",
    ").withColumn(\n",
    "    \"user_min_purchase_prior\",\n",
    "    _min(\"Purchase\").over(window_user)\n",
    ").withColumn(\n",
    "    \"user_max_purchase_prior\",\n",
    "    _max(\"Purchase\").over(window_user)\n",
    ").withColumn(\n",
    "    \"user_std_purchase_prior\",\n",
    "    stddev(\"Purchase\").over(window_user)\n",
    ")\n",
    "\n",
    "# Count distinct products (prior)\n",
    "# Note: countDistinct doesn't work with window rowsBetween, so we'll calculate differently\n",
    "# For now, we'll skip unique products/categories or use total count as proxy\n",
    "\n",
    "print(\"âœ“ User cumulative features added (excluding current transaction)\")\n",
    "print(\"\\nSample (first few transactions for each user):\")\n",
    "df_enriched.select(\n",
    "    \"User_ID\", \"transaction_order\", \"Purchase\",\n",
    "    \"user_purchase_count_prior\", \"user_avg_purchase_prior\", \"user_total_spent_prior\"\n",
    ").orderBy(\"User_ID\", \"transaction_order\").show(15)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ User cumulative features added (excluding current transaction)\n",
      "\n",
      "Sample (first few transactions for each user):\n",
      "+-------+-----------------+--------+-------------------------+-----------------------+----------------------+\n",
      "|User_ID|transaction_order|Purchase|user_purchase_count_prior|user_avg_purchase_prior|user_total_spent_prior|\n",
      "+-------+-----------------+--------+-------------------------+-----------------------+----------------------+\n",
      "|1000001|                1|   10900|                        0|                   NULL|                  NULL|\n",
      "|1000001|                2|    8839|                        1|                10900.0|                 10900|\n",
      "|1000001|                3|    9946|                        2|                 9869.5|                 19739|\n",
      "|1000001|                4|    2763|                        3|                 9895.0|                 29685|\n",
      "|1000001|                5|   15416|                        4|                 8112.0|                 32448|\n",
      "|1000001|                6|    8190|                        5|                 9572.8|                 47864|\n",
      "|1000001|                7|   13645|                        6|      9342.333333333334|                 56054|\n",
      "|1000001|                8|    6910|                        7|                 9957.0|                 69699|\n",
      "|1000001|                9|     612|                        8|               9576.125|                 76609|\n",
      "|1000001|               10|   13650|                        9|      8580.111111111111|                 77221|\n",
      "|1000001|               11|   11051|                       10|                 9087.1|                 90871|\n",
      "|1000001|               12|    2849|                       11|      9265.636363636364|                101922|\n",
      "|1000001|               13|    7943|                       12|      8730.916666666666|                104771|\n",
      "|1000001|               14|    1422|                       13|      8670.307692307691|                112714|\n",
      "|1000001|               15|   11011|                       14|      8152.571428571428|                114136|\n",
      "+-------+-----------------+--------+-------------------------+-----------------------+----------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:12:44.431309Z",
     "start_time": "2025-11-30T21:12:44.417682Z"
    }
   },
   "source": [
    "# Fill nulls for first transaction of each user\n",
    "# First transaction has no prior history, so we fill with defaults\n",
    "df_enriched = df_enriched.fillna({\n",
    "    'user_purchase_count_prior': 0,\n",
    "    'user_total_spent_prior': 0,\n",
    "    'user_avg_purchase_prior': 0,\n",
    "    'user_min_purchase_prior': 0,\n",
    "    'user_max_purchase_prior': 0,\n",
    "    'user_std_purchase_prior': 0\n",
    "})\n",
    "\n",
    "print(\"âœ“ Filled nulls for first transactions\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Filled nulls for first transactions\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Product Features (Excluding Current Transaction)\n",
    "\n",
    "**Approach:** \n",
    "1. Calculate total product stats (all transactions)\n",
    "2. For each row, subtract current transaction to get \"excluding current\"\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "product_avg_price_excl = (product_total_revenue - current_Purchase) / (product_count - 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:13:02.760396Z",
     "start_time": "2025-11-30T21:13:02.673393Z"
    }
   },
   "source": [
    "# Calculate TOTAL product stats (including all transactions)\n",
    "product_features_total = df_clean.groupBy(\"Product_ID\").agg(\n",
    "    count(\"*\").alias(\"product_total_count\"),\n",
    "    _sum(\"Purchase\").alias(\"product_total_revenue_all\"),\n",
    "    countDistinct(\"User_ID\").alias(\"product_unique_users\")\n",
    ")\n",
    "\n",
    "# Join back to main dataset\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(product_features_total),\n",
    "    on=\"Product_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate product stats EXCLUDING current transaction\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"product_purchase_count\",\n",
    "    col(\"product_total_count\") - 1  # Exclude current\n",
    ").withColumn(\n",
    "    \"product_total_revenue\",\n",
    "    col(\"product_total_revenue_all\") - col(\"Purchase\")  # Exclude current purchase\n",
    ").withColumn(\n",
    "    \"product_avg_price\",\n",
    "    when(col(\"product_total_count\") > 1,\n",
    "         (col(\"product_total_revenue_all\") - col(\"Purchase\")) / (col(\"product_total_count\") - 1)\n",
    "    ).otherwise(0)  # If only 1 purchase, no prior data\n",
    ")\n",
    "\n",
    "# Product popularity (based on count, not price)\n",
    "window_product_pop = Window.orderBy(col(\"product_purchase_count\").desc())\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"product_popularity_score\",\n",
    "    percent_rank().over(window_product_pop)\n",
    ")\n",
    "\n",
    "print(\"âœ“ Product features added (excluding current transaction)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Product features added (excluding current transaction)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Category Features (Excluding Current Transaction)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:13:08.357199Z",
     "start_time": "2025-11-30T21:13:08.312165Z"
    }
   },
   "source": [
    "# Calculate TOTAL category stats\n",
    "category_features_total = df_clean.groupBy(\"Product_Category_1\").agg(\n",
    "    count(\"*\").alias(\"category_total_count\"),\n",
    "    _sum(\"Purchase\").alias(\"category_total_revenue_all\")\n",
    ")\n",
    "\n",
    "# Join back\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(category_features_total),\n",
    "    on=\"Product_Category_1\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate category stats EXCLUDING current\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"category_purchase_count\",\n",
    "    col(\"category_total_count\") - 1\n",
    ").withColumn(\n",
    "    \"category_avg_price\",\n",
    "    when(col(\"category_total_count\") > 1,\n",
    "         (col(\"category_total_revenue_all\") - col(\"Purchase\")) / (col(\"category_total_count\") - 1)\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"âœ“ Category features added (excluding current transaction)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Category features added (excluding current transaction)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RFM Features (Frequency Only - No Monetary!)\n",
    "\n",
    "**Change:** We ONLY use frequency (purchase count), NOT monetary (total spent).\n",
    "\n",
    "Why? Monetary uses Purchase values, which causes data leakage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:13:48.483382Z",
     "start_time": "2025-11-30T21:13:48.465958Z"
    }
   },
   "source": [
    "# RFM: Only Frequency (based on prior purchase count)\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"rfm_frequency_score\",\n",
    "    percent_rank().over(Window.orderBy(col(\"user_purchase_count_prior\")))\n",
    ")\n",
    "\n",
    "# NOTE: We do NOT calculate rfm_monetary_score or rfm_score\n",
    "# because they would use Purchase values (data leakage)\n",
    "\n",
    "print(\"âœ“ RFM Frequency score added (NO monetary score - avoiding leakage)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RFM Frequency score added (NO monetary score - avoiding leakage)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interaction Features (Safe Only)\n",
    "\n",
    "**Changes:**\n",
    "- âŒ REMOVED: `purchase_vs_user_avg_ratio` (uses Purchase / user_avg)\n",
    "- âŒ REMOVED: `purchase_vs_product_avg_ratio` (uses Purchase / product_avg)\n",
    "- âŒ REMOVED: `is_above_user_avg` (compares Purchase with avg)\n",
    "- âœ… KEPT: `is_high_value_customer` (based on frequency only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:13:55.762274Z",
     "start_time": "2025-11-30T21:13:55.740041Z"
    }
   },
   "source": [
    "# High value customer flag (based on frequency, not monetary)\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"is_high_value_customer\",\n",
    "    when(col(\"rfm_frequency_score\") >= 0.8, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Derived features (safe)\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"user_purchase_range_prior\",\n",
    "    col(\"user_max_purchase_prior\") - col(\"user_min_purchase_prior\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ Safe interaction features added\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Safe interaction features added\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:14:02.761828Z",
     "start_time": "2025-11-30T21:13:59.640732Z"
    }
   },
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [\n",
    "    \"Gender\",\n",
    "    \"Age\",\n",
    "    \"City_Category\",\n",
    "    \"Stay_In_Current_City_Years\"\n",
    "]\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\", dropLast=False)\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "encoding_pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Fit and transform\n",
    "encoding_model = encoding_pipeline.fit(df_enriched)\n",
    "df_encoded = encoding_model.transform(df_enriched)\n",
    "\n",
    "print(\"âœ“ Categorical encoding completed\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Categorical encoding completed\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Selection for ML\n",
    "\n",
    "**Key changes:**\n",
    "- Use `_prior` versions of user features (computed from previous transactions only)\n",
    "- Use product/category features that exclude current transaction\n",
    "- NO monetary-based RFM features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:14:15.402092Z",
     "start_time": "2025-11-30T21:14:15.398575Z"
    }
   },
   "source": [
    "# Select features for ML models\n",
    "feature_cols = [\n",
    "    # Categorical (indexed)\n",
    "    \"Gender_index\", \"Age_index\", \"City_Category_index\",\n",
    "    \"Stay_In_Current_City_Years_index\",\n",
    "    \n",
    "    # Numerical demographics\n",
    "    \"Occupation\", \"Marital_Status\",\n",
    "    \n",
    "    # Product categories\n",
    "    \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\",\n",
    "    \n",
    "    # User features (PRIOR - from previous transactions)\n",
    "    \"user_purchase_count_prior\",\n",
    "    \"user_total_spent_prior\",\n",
    "    \"user_avg_purchase_prior\",\n",
    "    \"user_std_purchase_prior\",\n",
    "    \"user_min_purchase_prior\",\n",
    "    \"user_max_purchase_prior\",\n",
    "    \"user_purchase_range_prior\",\n",
    "    \n",
    "    # Product features (excluding current)\n",
    "    \"product_purchase_count\",\n",
    "    \"product_unique_users\",\n",
    "    \"product_avg_price\",\n",
    "    \"product_popularity_score\",\n",
    "    \n",
    "    # Category features (excluding current)\n",
    "    \"category_purchase_count\",\n",
    "    \"category_avg_price\",\n",
    "    \n",
    "    # RFM features (frequency only)\n",
    "    \"rfm_frequency_score\",\n",
    "    \n",
    "    # Interaction features\n",
    "    \"is_high_value_customer\"\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(\"\\nFeatures:\")\n",
    "for i, f in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 24\n",
      "\n",
      "Features:\n",
      "  1. Gender_index\n",
      "  2. Age_index\n",
      "  3. City_Category_index\n",
      "  4. Stay_In_Current_City_Years_index\n",
      "  5. Occupation\n",
      "  6. Marital_Status\n",
      "  7. Product_Category_1\n",
      "  8. Product_Category_2\n",
      "  9. Product_Category_3\n",
      "  10. user_purchase_count_prior\n",
      "  11. user_total_spent_prior\n",
      "  12. user_avg_purchase_prior\n",
      "  13. user_std_purchase_prior\n",
      "  14. user_min_purchase_prior\n",
      "  15. user_max_purchase_prior\n",
      "  16. user_purchase_range_prior\n",
      "  17. product_purchase_count\n",
      "  18. product_unique_users\n",
      "  19. product_avg_price\n",
      "  20. product_popularity_score\n",
      "  21. category_purchase_count\n",
      "  22. category_avg_price\n",
      "  23. rfm_frequency_score\n",
      "  24. is_high_value_customer\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:14:32.140588Z",
     "start_time": "2025-11-30T21:14:32.088457Z"
    }
   },
   "source": [
    "# Fill any remaining nulls with 0\n",
    "df_encoded = df_encoded.fillna(0, subset=feature_cols)\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df_features = assembler.transform(df_encoded)\n",
    "\n",
    "print(f\"âœ“ Feature vector assembled with {len(feature_cols)} features\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Feature vector assembled with 24 features\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:14:40.178576Z",
     "start_time": "2025-11-30T21:14:34.737013Z"
    }
   },
   "source": [
    "# Standard Scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=False,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_scaled = scaler_model.transform(df_features)\n",
    "\n",
    "print(\"âœ“ Features scaled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features scaled\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:14:46.503209Z",
     "start_time": "2025-11-30T21:14:42.432822Z"
    }
   },
   "source": [
    "# Select final columns\n",
    "final_cols = [\n",
    "    # IDs\n",
    "    \"User_ID\", \"Product_ID\",\n",
    "    \n",
    "    # Target\n",
    "    \"Purchase\",\n",
    "    \n",
    "    # Original features\n",
    "    \"Gender\", \"Age\", \"Occupation\", \"City_Category\",\n",
    "    \"Stay_In_Current_City_Years\", \"Marital_Status\",\n",
    "    \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\",\n",
    "    \n",
    "    # Transaction order\n",
    "    \"transaction_order\",\n",
    "    \n",
    "    # Engineered features (prior/excluding current)\n",
    "    \"user_purchase_count_prior\", \"user_total_spent_prior\", \"user_avg_purchase_prior\",\n",
    "    \"user_std_purchase_prior\", \"user_min_purchase_prior\", \"user_max_purchase_prior\",\n",
    "    \"user_purchase_range_prior\",\n",
    "    \"product_purchase_count\", \"product_unique_users\", \"product_avg_price\",\n",
    "    \"product_popularity_score\",\n",
    "    \"category_purchase_count\", \"category_avg_price\",\n",
    "    \"rfm_frequency_score\", \"is_high_value_customer\",\n",
    "    \n",
    "    # ML-ready features\n",
    "    \"features\", \"scaled_features\"\n",
    "]\n",
    "\n",
    "df_final = df_scaled.select(*final_cols)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE (NO DATA LEAKAGE)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final dataset: {df_final.count():,} rows x {len(df_final.columns)} columns\")\n",
    "print(f\"Total features in vector: {len(feature_cols)}\")\n",
    "print(\"\\nâœ… All features computed WITHOUT using current transaction!\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE (NO DATA LEAKAGE)\n",
      "============================================================\n",
      "Final dataset: 550,068 rows x 30 columns\n",
      "Total features in vector: 24\n",
      "\n",
      "âœ… All features computed WITHOUT using current transaction!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:14:57.608187Z",
     "start_time": "2025-11-30T21:14:53.942897Z"
    }
   },
   "source": [
    "# Show sample\n",
    "print(\"\\nSample data (first user's transactions):\")\n",
    "df_final.filter(col(\"User_ID\") == 1000001).select(\n",
    "    \"User_ID\", \"transaction_order\", \"Purchase\",\n",
    "    \"user_purchase_count_prior\", \"user_avg_purchase_prior\",\n",
    "    \"product_avg_price\", \"category_avg_price\"\n",
    ").show(10, truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data (first user's transactions):\n",
      "+-------+-----------------+--------+-------------------------+-----------------------+------------------+------------------+\n",
      "|User_ID|transaction_order|Purchase|user_purchase_count_prior|user_avg_purchase_prior|product_avg_price |category_avg_price|\n",
      "+-------+-----------------+--------+-------------------------+-----------------------+------------------+------------------+\n",
      "|1000001|1                |10900   |0                        |0.0                    |11445.199057714959|10096.665990500693|\n",
      "|1000001|2                |8839    |1                        |10900.0                |6975.126192223038 |6240.070959107413 |\n",
      "|1000001|3                |9946    |2                        |9869.5                 |8014.921458625526 |7498.9365980829325|\n",
      "|1000001|4                |2763    |3                        |9895.0                 |2790.9124497991966|2329.622617426821 |\n",
      "|1000001|5                |15416   |4                        |8112.0                 |17335.656753407682|13606.205703213489|\n",
      "|1000001|6                |8190    |5                        |9572.8                 |9453.331010452961 |10096.800069265782|\n",
      "|1000001|7                |13645   |6                        |9342.333333333334      |8029.382775119617 |10096.530180091035|\n",
      "|1000001|8                |6910    |7                        |9957.0                 |7002.726895119418 |6240.0837396973475|\n",
      "|1000001|9                |612     |8                        |9576.125               |373.97416974169744|370.3864260494311 |\n",
      "|1000001|10               |13650   |9                        |8580.111111111111      |11141.464813205908|10096.52993271324 |\n",
      "+-------+-----------------+--------+-------------------------+-----------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Validation: Check for Data Leakage\n",
    "\n",
    "Let's verify that our features don't leak information about the target."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:15:27.102286Z",
     "start_time": "2025-11-30T21:15:17.541151Z"
    }
   },
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VALIDATION: Checking for Data Leakage\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: For first transaction of each user, prior features should be 0\n",
    "first_transactions = df_final.filter(col(\"transaction_order\") == 1)\n",
    "print(f\"\\nâœ“ Check 1: First transactions for each user\")\n",
    "print(f\"  Count: {first_transactions.count():,}\")\n",
    "print(\"  Prior features should be 0:\")\n",
    "first_transactions.select(\n",
    "    \"User_ID\", \"Purchase\",\n",
    "    \"user_purchase_count_prior\", \"user_avg_purchase_prior\"\n",
    ").show(5)\n",
    "\n",
    "# Check 2: For subsequent transactions, prior avg should NOT equal current Purchase\n",
    "subsequent_transactions = df_final.filter(col(\"transaction_order\") > 1).sample(False, 0.001)\n",
    "print(\"\\nâœ“ Check 2: Subsequent transactions (sample)\")\n",
    "print(\"  user_avg_purchase_prior should NOT equal Purchase:\")\n",
    "subsequent_transactions.select(\n",
    "    \"User_ID\", \"transaction_order\", \"Purchase\",\n",
    "    \"user_avg_purchase_prior\"\n",
    ").withColumn(\n",
    "    \"are_equal\",\n",
    "    when(col(\"Purchase\") == col(\"user_avg_purchase_prior\"), \"EQUAL\").otherwise(\"DIFFERENT\")\n",
    ").show(10)\n",
    "\n",
    "print(\"\\nâœ… Validation complete!\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATION: Checking for Data Leakage\n",
      "============================================================\n",
      "\n",
      "âœ“ Check 1: First transactions for each user\n",
      "  Count: 5,891\n",
      "  Prior features should be 0:\n",
      "+-------+--------+-------------------------+-----------------------+\n",
      "|User_ID|Purchase|user_purchase_count_prior|user_avg_purchase_prior|\n",
      "+-------+--------+-------------------------+-----------------------+\n",
      "|1003034|    8905|                        0|                    0.0|\n",
      "|1003020|    8732|                        0|                    0.0|\n",
      "|1006015|    8755|                        0|                    0.0|\n",
      "|1004046|    8888|                        0|                    0.0|\n",
      "|1003229|    8895|                        0|                    0.0|\n",
      "+-------+--------+-------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "âœ“ Check 2: Subsequent transactions (sample)\n",
      "  user_avg_purchase_prior should NOT equal Purchase:\n",
      "+-------+-----------------+--------+-----------------------+---------+\n",
      "|User_ID|transaction_order|Purchase|user_avg_purchase_prior|are_equal|\n",
      "+-------+-----------------+--------+-----------------------+---------+\n",
      "|1003973|                2|    8881|                15192.0|DIFFERENT|\n",
      "|1004384|                2|   10796|                15684.0|DIFFERENT|\n",
      "|1004695|                2|    5362|                 4579.0|DIFFERENT|\n",
      "|1003013|                2|    7033|                13413.0|DIFFERENT|\n",
      "|1003205|                2|   10854|                11763.0|DIFFERENT|\n",
      "|1005420|                2|   12971|                11032.0|DIFFERENT|\n",
      "|1001671|                2|   16528|                 8821.0|DIFFERENT|\n",
      "|1006003|                2|   20463|                11654.0|DIFFERENT|\n",
      "|1000830|                2|    7980|                 7991.0|DIFFERENT|\n",
      "|1003235|                3|   13196|                13083.5|DIFFERENT|\n",
      "+-------+-----------------+--------+-----------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "âœ… Validation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:16:04.973216Z",
     "start_time": "2025-11-30T21:15:55.776809Z"
    }
   },
   "source": [
    "# Save to Delta Lake (separate location to avoid overwriting original)\n",
    "delta_path = \"../data/processed/delta/features_fixed\"\n",
    "\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"\\nâœ“ Data saved to Delta Lake: {delta_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… FEATURE ENGINEERING FIXED - NO DATA LEAKAGE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nðŸ“Š Summary of Changes:\")\n",
    "print(\"  âœ“ User features: computed from PRIOR transactions only\")\n",
    "print(\"  âœ“ Product features: exclude current transaction\")\n",
    "print(\"  âœ“ Category features: exclude current transaction\")\n",
    "print(\"  âœ“ RFM: frequency only (NO monetary)\")\n",
    "print(\"  âœ“ Removed: purchase_vs_*_ratio features (contained Purchase)\")\n",
    "print(\"\\nðŸŽ¯ Next Step: Train models on features_fixed dataset\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Data saved to Delta Lake: ../data/processed/delta/features_fixed\n",
      "\n",
      "============================================================\n",
      "âœ… FEATURE ENGINEERING FIXED - NO DATA LEAKAGE\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Summary of Changes:\n",
      "  âœ“ User features: computed from PRIOR transactions only\n",
      "  âœ“ Product features: exclude current transaction\n",
      "  âœ“ Category features: exclude current transaction\n",
      "  âœ“ RFM: frequency only (NO monetary)\n",
      "  âœ“ Removed: purchase_vs_*_ratio features (contained Purchase)\n",
      "\n",
      "ðŸŽ¯ Next Step: Train models on features_fixed dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
