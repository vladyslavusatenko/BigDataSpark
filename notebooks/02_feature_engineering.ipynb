{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline - Black Friday Sales\n",
    "\n",
    "**Cel:** Stworzenie zaawansowanych cech dla modeli Machine Learning\n",
    "\n",
    "**Pipeline:**\n",
    "1. Åadowanie danych surowych\n",
    "2. Feature Engineering:\n",
    "   - Agregacje per uÅ¼ytkownik\n",
    "   - Agregacje per produkt\n",
    "   - RFM Features\n",
    "   - Categorical Encoding\n",
    "   - Interakcje cech\n",
    "3. Zapis do Delta Lake\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as _sum, avg, min as _min, max as _max,\n",
    "    countDistinct, stddev, when, lit, dense_rank, percent_rank,\n",
    "    broadcast, concat_ws, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, MinMaxScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from config.spark_config import SparkConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicjalizacja Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "spark = SparkConfig.get_spark_session(\"BlackFriday-FeatureEngineering\")\n",
    "\n",
    "print(\"\\nâœ“ Spark session ready!\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Åadowanie Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_raw = spark.read.csv(\n",
    "    \"../data/raw/BlackFriday.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {df_raw.count():,} rows x {len(df_raw.columns)} columns\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "total_rows = df_raw.count()\n",
    "unique_rows = df_raw.dropDuplicates().count()\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Unique rows: {unique_rows:,}\")\n",
    "print(f\"Duplicates: {total_rows - unique_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "\n",
    "missing_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_raw.columns\n",
    "])\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df_clean = df_raw.dropDuplicates()\n",
    "\n",
    "# Fill missing Product_Category_2 and Product_Category_3 with 0 (indicating no secondary category)\n",
    "df_clean = df_clean.fillna({\n",
    "    'Product_Category_2': 0,\n",
    "    'Product_Category_3': 0\n",
    "})\n",
    "\n",
    "print(f\"\\nâœ“ Cleaned data: {df_clean.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - User-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User aggregations\n",
    "user_features = df_clean.groupBy(\"User_ID\").agg(\n",
    "    # Purchase statistics\n",
    "    count(\"*\").alias(\"user_purchase_count\"),\n",
    "    _sum(\"Purchase\").alias(\"user_total_spent\"),\n",
    "    avg(\"Purchase\").alias(\"user_avg_purchase\"),\n",
    "    _min(\"Purchase\").alias(\"user_min_purchase\"),\n",
    "    _max(\"Purchase\").alias(\"user_max_purchase\"),\n",
    "    stddev(\"Purchase\").alias(\"user_std_purchase\"),\n",
    "    \n",
    "    # Product diversity\n",
    "    countDistinct(\"Product_ID\").alias(\"user_unique_products\"),\n",
    "    countDistinct(\"Product_Category_1\").alias(\"user_unique_categories\"),\n",
    "    \n",
    "    # Categorical modes (most frequent)\n",
    "    # We'll take the first value as a proxy for mode\n",
    ")\n",
    "\n",
    "# Add derived features\n",
    "user_features = user_features.withColumn(\n",
    "    \"user_purchase_range\",\n",
    "    col(\"user_max_purchase\") - col(\"user_min_purchase\")\n",
    ")\n",
    "\n",
    "user_features = user_features.withColumn(\n",
    "    \"user_avg_products_per_transaction\",\n",
    "    col(\"user_unique_products\") / col(\"user_purchase_count\")\n",
    ")\n",
    "\n",
    "print(f\"\\nUser features created: {user_features.count():,} users\")\n",
    "user_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering - Product-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product aggregations\n",
    "product_features = df_clean.groupBy(\"Product_ID\").agg(\n",
    "    count(\"*\").alias(\"product_purchase_count\"),\n",
    "    countDistinct(\"User_ID\").alias(\"product_unique_users\"),\n",
    "    _sum(\"Purchase\").alias(\"product_total_revenue\"),\n",
    "    avg(\"Purchase\").alias(\"product_avg_price\"),\n",
    "    _min(\"Purchase\").alias(\"product_min_price\"),\n",
    "    _max(\"Purchase\").alias(\"product_max_price\"),\n",
    "    stddev(\"Purchase\").alias(\"product_std_price\")\n",
    ")\n",
    "\n",
    "# Product popularity ranking\n",
    "window_spec = Window.orderBy(col(\"product_purchase_count\").desc())\n",
    "product_features = product_features.withColumn(\n",
    "    \"product_popularity_rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "# Normalize popularity to 0-1 scale\n",
    "product_features = product_features.withColumn(\n",
    "    \"product_popularity_score\",\n",
    "    percent_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "print(f\"\\nProduct features created: {product_features.count():,} products\")\n",
    "product_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering - Category-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category aggregations\n",
    "category_features = df_clean.groupBy(\"Product_Category_1\").agg(\n",
    "    count(\"*\").alias(\"category_purchase_count\"),\n",
    "    avg(\"Purchase\").alias(\"category_avg_price\"),\n",
    "    _sum(\"Purchase\").alias(\"category_total_revenue\")\n",
    ")\n",
    "\n",
    "print(f\"\\nCategory features: {category_features.count()} categories\")\n",
    "category_features.orderBy(col(\"category_total_revenue\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RFM Features (Recency, Frequency, Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RFM features\n",
    "# Note: Dataset doesn't have timestamps, so we'll create proxy RFM based on transaction ordering\n",
    "\n",
    "# Frequency: Already have user_purchase_count\n",
    "# Monetary: Already have user_total_spent\n",
    "# Recency: Use row_number as proxy for time (last purchase)\n",
    "\n",
    "window_user = Window.partitionBy(\"User_ID\").orderBy(col(\"Purchase\").desc())\n",
    "\n",
    "df_with_recency = df_clean.withColumn(\n",
    "    \"transaction_order\",\n",
    "    row_number().over(window_user)\n",
    ")\n",
    "\n",
    "# Get recency (1 = most recent for each user)\n",
    "recency_features = df_with_recency.filter(col(\"transaction_order\") == 1).select(\n",
    "    \"User_ID\",\n",
    "    col(\"Purchase\").alias(\"last_purchase_amount\")\n",
    ")\n",
    "\n",
    "# Join recency with user features to create full RFM\n",
    "rfm_features = user_features.join(\n",
    "    recency_features,\n",
    "    on=\"User_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# RFM Scoring (quintiles)\n",
    "rfm_features = rfm_features.withColumn(\n",
    "    \"rfm_frequency_score\",\n",
    "    percent_rank().over(Window.orderBy(col(\"user_purchase_count\")))\n",
    ")\n",
    "\n",
    "rfm_features = rfm_features.withColumn(\n",
    "    \"rfm_monetary_score\",\n",
    "    percent_rank().over(Window.orderBy(col(\"user_total_spent\")))\n",
    ")\n",
    "\n",
    "# Combined RFM score\n",
    "rfm_features = rfm_features.withColumn(\n",
    "    \"rfm_score\",\n",
    "    (col(\"rfm_frequency_score\") + col(\"rfm_monetary_score\")) / 2\n",
    ")\n",
    "\n",
    "print(\"\\nRFM Features:\")\n",
    "rfm_features.select(\n",
    "    \"User_ID\", \"user_purchase_count\", \"user_total_spent\",\n",
    "    \"rfm_frequency_score\", \"rfm_monetary_score\", \"rfm_score\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Join All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join user features back to main dataset\n",
    "df_enriched = df_clean.join(\n",
    "    broadcast(user_features),\n",
    "    on=\"User_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join product features\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(product_features),\n",
    "    on=\"Product_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join category features\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(category_features),\n",
    "    on=\"Product_Category_1\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join RFM features\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(rfm_features.select(\n",
    "        \"User_ID\", \"rfm_frequency_score\", \"rfm_monetary_score\", \"rfm_score\"\n",
    "    )),\n",
    "    on=\"User_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Enriched dataset: {df_enriched.count():,} rows x {len(df_enriched.columns)} columns\")\n",
    "print(\"\\nNew features added:\")\n",
    "print([c for c in df_enriched.columns if c not in df_clean.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"purchase_vs_user_avg_ratio\",\n",
    "    col(\"Purchase\") / col(\"user_avg_purchase\")\n",
    ")\n",
    "\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"purchase_vs_product_avg_ratio\",\n",
    "    col(\"Purchase\") / col(\"product_avg_price\")\n",
    ")\n",
    "\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"purchase_vs_category_avg_ratio\",\n",
    "    col(\"Purchase\") / col(\"category_avg_price\")\n",
    ")\n",
    "\n",
    "# Is this purchase above user's average?\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"is_above_user_avg\",\n",
    "    when(col(\"Purchase\") > col(\"user_avg_purchase\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# High value customer flag (top 20%)\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"is_high_value_customer\",\n",
    "    when(col(\"rfm_score\") >= 0.8, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Interaction features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [\n",
    "    \"Gender\",\n",
    "    \"Age\",\n",
    "    \"City_Category\",\n",
    "    \"Stay_In_Current_City_Years\"\n",
    "]\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\", dropLast=False)\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "encoding_pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Fit and transform\n",
    "encoding_model = encoding_pipeline.fit(df_enriched)\n",
    "df_encoded = encoding_model.transform(df_enriched)\n",
    "\n",
    "print(\"\\nâœ“ Categorical encoding completed\")\n",
    "print(f\"Total columns after encoding: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Selection for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for ML models\n",
    "feature_cols = [\n",
    "    # Categorical (indexed)\n",
    "    \"Gender_index\", \"Age_index\", \"City_Category_index\",\n",
    "    \"Stay_In_Current_City_Years_index\",\n",
    "    \n",
    "    # Numerical demographics\n",
    "    \"Occupation\", \"Marital_Status\",\n",
    "    \n",
    "    # Product categories\n",
    "    \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\",\n",
    "    \n",
    "    # User features\n",
    "    \"user_purchase_count\", \"user_total_spent\", \"user_avg_purchase\",\n",
    "    \"user_std_purchase\", \"user_unique_products\", \"user_unique_categories\",\n",
    "    \n",
    "    # Product features\n",
    "    \"product_purchase_count\", \"product_unique_users\", \"product_avg_price\",\n",
    "    \"product_popularity_score\",\n",
    "    \n",
    "    # Category features\n",
    "    \"category_avg_price\",\n",
    "    \n",
    "    # RFM features\n",
    "    \"rfm_frequency_score\", \"rfm_monetary_score\", \"rfm_score\",\n",
    "    \n",
    "    # Interaction features\n",
    "    \"purchase_vs_user_avg_ratio\", \"purchase_vs_product_avg_ratio\",\n",
    "    \"is_above_user_avg\", \"is_high_value_customer\"\n",
    "]\n",
    "\n",
    "# Fill any remaining nulls with 0\n",
    "df_encoded = df_encoded.fillna(0, subset=feature_cols)\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df_features = assembler.transform(df_encoded)\n",
    "\n",
    "print(f\"\\nâœ“ Feature vector assembled with {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=False,  # Sparse vectors don't support centering\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_scaled = scaler_model.transform(df_features)\n",
    "\n",
    "print(\"\\nâœ“ Features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Feature Set Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final columns for ML\n",
    "final_cols = [\n",
    "    # IDs\n",
    "    \"User_ID\", \"Product_ID\",\n",
    "    \n",
    "    # Target\n",
    "    \"Purchase\",\n",
    "    \n",
    "    # Original features\n",
    "    \"Gender\", \"Age\", \"Occupation\", \"City_Category\",\n",
    "    \"Stay_In_Current_City_Years\", \"Marital_Status\",\n",
    "    \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\",\n",
    "    \n",
    "    # Engineered features (all)\n",
    "] + [c for c in df_scaled.columns if c.startswith((\"user_\", \"product_\", \"category_\", \"rfm_\", \"is_\", \"purchase_vs\"))] + [\n",
    "    # ML-ready features\n",
    "    \"features\", \"scaled_features\"\n",
    "]\n",
    "\n",
    "df_final = df_scaled.select(*final_cols)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final dataset: {df_final.count():,} rows x {len(df_final.columns)} columns\")\n",
    "print(f\"\\nTotal features in vector: {len(feature_cols)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(f\"  - User features: {len([c for c in feature_cols if c.startswith('user_')])}\")\n",
    "print(f\"  - Product features: {len([c for c in feature_cols if c.startswith('product_')])}\")\n",
    "print(f\"  - Category features: {len([c for c in feature_cols if c.startswith('category_')])}\")\n",
    "print(f\"  - RFM features: {len([c for c in feature_cols if c.startswith('rfm_')])}\")\n",
    "print(f\"  - Interaction features: {len([c for c in feature_cols if 'ratio' in c or 'is_' in c])}\")\n",
    "print(f\"  - Demographics: {len([c for c in feature_cols if c in categorical_cols or c in ['Occupation', 'Marital_Status']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "df_final.select(\n",
    "    \"User_ID\", \"Product_ID\", \"Purchase\",\n",
    "    \"user_avg_purchase\", \"product_popularity_score\", \"rfm_score\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Delta Lake\n",
    "delta_path = \"../data/processed/delta/features\"\n",
    "\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"City_Category\", \"Age\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"\\nâœ“ Data saved to Delta Lake: {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual feature sets for specific use cases\n",
    "\n",
    "# 1. User-level features (for segmentation)\n",
    "rfm_features.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"../data/processed/delta/user_features\")\n",
    "\n",
    "print(\"âœ“ User features saved\")\n",
    "\n",
    "# 2. Product-level features (for recommendations)\n",
    "product_features.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"../data/processed/delta/product_features\")\n",
    "\n",
    "print(\"âœ“ Product features saved\")\n",
    "\n",
    "# 3. Encoding pipeline model\n",
    "encoding_model.write().overwrite().save(\"../models/encoding_pipeline\")\n",
    "\n",
    "print(\"âœ“ Encoding pipeline saved\")\n",
    "\n",
    "# 4. Scaler model\n",
    "scaler_model.write().overwrite().save(\"../models/scaler_model\")\n",
    "\n",
    "print(\"âœ“ Scaler model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Validation & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select numerical features for stats\n",
    "numerical_features = [\n",
    "    \"Purchase\",\n",
    "    \"user_avg_purchase\", \"user_purchase_count\", \"user_total_spent\",\n",
    "    \"product_popularity_score\", \"product_avg_price\",\n",
    "    \"rfm_score\"\n",
    "]\n",
    "\n",
    "df_final.select(numerical_features).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in final dataset\n",
    "null_counts = df_final.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_final.columns[:20]  # First 20 cols\n",
    "])\n",
    "\n",
    "print(\"\\nNull counts in final dataset (first 20 columns):\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target (Purchase)\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = Correlation.corr(df_final, \"scaled_features\").head()\n",
    "\n",
    "print(\"\\nâœ“ Correlation matrix computed\")\n",
    "print(\"(Can be used for feature selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… FEATURE ENGINEERING PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š Created Features:\")\n",
    "print(\"  âœ“ User-level aggregations (purchase patterns)\")\n",
    "print(\"  âœ“ Product-level aggregations (popularity, pricing)\")\n",
    "print(\"  âœ“ Category-level aggregations\")\n",
    "print(\"  âœ“ RFM features (Frequency, Monetary)\")\n",
    "print(\"  âœ“ Interaction features (ratios, flags)\")\n",
    "print(\"  âœ“ Categorical encoding (StringIndexer + OneHot)\")\n",
    "print(\"  âœ“ Feature scaling (StandardScaler)\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved Artifacts:\")\n",
    "print(\"  âœ“ Main features dataset â†’ data/processed/delta/features/\")\n",
    "print(\"  âœ“ User features â†’ data/processed/delta/user_features/\")\n",
    "print(\"  âœ“ Product features â†’ data/processed/delta/product_features/\")\n",
    "print(\"  âœ“ Encoding pipeline â†’ models/encoding_pipeline/\")\n",
    "print(\"  âœ“ Scaler model â†’ models/scaler_model/\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"  1. Build ETL pipeline (automation)\")\n",
    "print(\"  2. Train ML models (regression, clustering, recommendation)\")\n",
    "print(\"  3. Model evaluation and tuning\")\n",
    "print(\"  4. Setup Kafka + Streaming pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "# print(\"\\nâœ“ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
