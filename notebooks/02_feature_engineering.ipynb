{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline - Black Friday Sales\n",
    "\n",
    "**Cel:** Stworzenie zaawansowanych cech dla modeli Machine Learning\n",
    "\n",
    "**Pipeline:**\n",
    "1. Åadowanie danych surowych\n",
    "2. Feature Engineering:\n",
    "   - Agregacje per uÅ¼ytkownik\n",
    "   - Agregacje per produkt\n",
    "   - RFM Features\n",
    "   - Categorical Encoding\n",
    "   - Interakcje cech\n",
    "3. Zapis do Delta Lake\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:34.784543Z",
     "start_time": "2025-11-25T21:51:34.614219Z"
    }
   },
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as _sum, avg, min as _min, max as _max,\n",
    "    countDistinct, stddev, when, lit, dense_rank, percent_rank,\n",
    "    broadcast, concat_ws, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, MinMaxScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from config.spark_config import SparkConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicjalizacja Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:37.889108Z",
     "start_time": "2025-11-25T21:51:34.789216Z"
    }
   },
   "source": [
    "# Start Spark Session\n",
    "spark = SparkConfig.get_spark_session(\"BlackFriday-FeatureEngineering\")\n",
    "\n",
    "print(\"\\nâœ“ Spark session ready!\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Windows: HADOOP_HOME set to C:\\Users\\usate\\PycharmProjects\\BlackFriday\\hadoop\n",
      "  Note: If you encounter permission errors, download winutils.exe\n",
      "  from https://github.com/steveloughran/winutils and place in hadoop/bin/\n",
      "============================================================\n",
      "Spark Session Created: BlackFriday-FeatureEngineering\n",
      "============================================================\n",
      "Spark Version: 3.5.3\n",
      "Master: local[*]\n",
      "App Name: BlackFriday-FeatureEngineering\n",
      "============================================================\n",
      "\n",
      "âœ“ Spark session ready!\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Åadowanie Danych"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:41.322541Z",
     "start_time": "2025-11-25T21:51:38.046890Z"
    }
   },
   "source": [
    "# Load raw data\n",
    "df_raw = spark.read.csv(\n",
    "    \"../data/raw/train.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {df_raw.count():,} rows x {len(df_raw.columns)} columns\")\n",
    "df_raw.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: 550,068 rows x 12 columns\n",
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:41.459961Z",
     "start_time": "2025-11-25T21:51:41.353479Z"
    }
   },
   "source": [
    "# Show sample\n",
    "df_raw.show(5, truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age |Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001|P00069042 |F     |0-17|10        |A            |2                         |0             |3                 |NULL              |NULL              |8370    |\n",
      "|1000001|P00248942 |F     |0-17|10        |A            |2                         |0             |1                 |6                 |14                |15200   |\n",
      "|1000001|P00087842 |F     |0-17|10        |A            |2                         |0             |12                |NULL              |NULL              |1422    |\n",
      "|1000001|P00085442 |F     |0-17|10        |A            |2                         |0             |12                |14                |NULL              |1057    |\n",
      "|1000002|P00285442 |M     |55+ |16        |C            |4+                        |0             |8                 |NULL              |NULL              |7969    |\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:42.605041Z",
     "start_time": "2025-11-25T21:51:41.479807Z"
    }
   },
   "source": [
    "# Check for duplicates\n",
    "total_rows = df_raw.count()\n",
    "unique_rows = df_raw.dropDuplicates().count()\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Unique rows: {unique_rows:,}\")\n",
    "print(f\"Duplicates: {total_rows - unique_rows:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 550,068\n",
      "Unique rows: 550,068\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:42.938357Z",
     "start_time": "2025-11-25T21:51:42.630211Z"
    }
   },
   "source": [
    "# Check missing values\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "\n",
    "missing_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_raw.columns\n",
    "])\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_counts.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|      0|         0|     0|  0|         0|            0|                         0|             0|                 0|            173638|            383247|       0|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:43.614327Z",
     "start_time": "2025-11-25T21:51:42.946723Z"
    }
   },
   "source": [
    "# Clean data\n",
    "df_clean = df_raw.dropDuplicates()\n",
    "\n",
    "# Fill missing Product_Category_2 and Product_Category_3 with 0 (indicating no secondary category)\n",
    "df_clean = df_clean.fillna({\n",
    "    'Product_Category_2': 0,\n",
    "    'Product_Category_3': 0\n",
    "})\n",
    "\n",
    "print(f\"\\nâœ“ Cleaned data: {df_clean.count():,} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Cleaned data: 550,068 rows\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - User-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:47.277049Z",
     "start_time": "2025-11-25T21:51:43.622892Z"
    }
   },
   "source": [
    "# User aggregations\n",
    "user_features = df_clean.groupBy(\"User_ID\").agg(\n",
    "    # Purchase statistics\n",
    "    count(\"*\").alias(\"user_purchase_count\"),\n",
    "    _sum(\"Purchase\").alias(\"user_total_spent\"),\n",
    "    avg(\"Purchase\").alias(\"user_avg_purchase\"),\n",
    "    _min(\"Purchase\").alias(\"user_min_purchase\"),\n",
    "    _max(\"Purchase\").alias(\"user_max_purchase\"),\n",
    "    stddev(\"Purchase\").alias(\"user_std_purchase\"),\n",
    "    \n",
    "    # Product diversity\n",
    "    countDistinct(\"Product_ID\").alias(\"user_unique_products\"),\n",
    "    countDistinct(\"Product_Category_1\").alias(\"user_unique_categories\"),\n",
    "    \n",
    "    # Categorical modes (most frequent)\n",
    "    # We'll take the first value as a proxy for mode\n",
    ")\n",
    "\n",
    "# Add derived features\n",
    "user_features = user_features.withColumn(\n",
    "    \"user_purchase_range\",\n",
    "    col(\"user_max_purchase\") - col(\"user_min_purchase\")\n",
    ")\n",
    "\n",
    "user_features = user_features.withColumn(\n",
    "    \"user_avg_products_per_transaction\",\n",
    "    col(\"user_unique_products\") / col(\"user_purchase_count\")\n",
    ")\n",
    "\n",
    "print(f\"\\nUser features created: {user_features.count():,} users\")\n",
    "user_features.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User features created: 5,891 users\n",
      "+-------+-------------------+----------------+-----------------+-----------------+-----------------+-----------------+--------------------+----------------------+-------------------+---------------------------------+\n",
      "|User_ID|user_purchase_count|user_total_spent|user_avg_purchase|user_min_purchase|user_max_purchase|user_std_purchase|user_unique_products|user_unique_categories|user_purchase_range|user_avg_products_per_transaction|\n",
      "+-------+-------------------+----------------+-----------------+-----------------+-----------------+-----------------+--------------------+----------------------+-------------------+---------------------------------+\n",
      "|1003031|                290|         2585988|           8917.2|               49|            20855|3957.360091540527|                 290|                    14|              20806|                              1.0|\n",
      "|1000190|                 81|          778581|9612.111111111111|               24|            23296|5108.730825753105|                  81|                    11|              23272|                              1.0|\n",
      "|1000149|                334|         3775096|11302.68263473054|              494|            23144|5023.001391604217|                 334|                    16|              22650|                              1.0|\n",
      "|1005853|                 83|          784192|9448.096385542169|              391|            23423|5984.178574383348|                  83|                    10|              23032|                              1.0|\n",
      "|1004739|                 62|          573581|9251.306451612903|               25|            19492|4646.306658011884|                  62|                     9|              19467|                              1.0|\n",
      "+-------+-------------------+----------------+-----------------+-----------------+-----------------+-----------------+--------------------+----------------------+-------------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering - Product-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:50.830356Z",
     "start_time": "2025-11-25T21:51:47.309718Z"
    }
   },
   "source": [
    "# Product aggregations\n",
    "product_features = df_clean.groupBy(\"Product_ID\").agg(\n",
    "    count(\"*\").alias(\"product_purchase_count\"),\n",
    "    countDistinct(\"User_ID\").alias(\"product_unique_users\"),\n",
    "    _sum(\"Purchase\").alias(\"product_total_revenue\"),\n",
    "    avg(\"Purchase\").alias(\"product_avg_price\"),\n",
    "    _min(\"Purchase\").alias(\"product_min_price\"),\n",
    "    _max(\"Purchase\").alias(\"product_max_price\"),\n",
    "    stddev(\"Purchase\").alias(\"product_std_price\")\n",
    ")\n",
    "\n",
    "# Product popularity ranking\n",
    "window_spec = Window.orderBy(col(\"product_purchase_count\").desc())\n",
    "product_features = product_features.withColumn(\n",
    "    \"product_popularity_rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "# Normalize popularity to 0-1 scale\n",
    "product_features = product_features.withColumn(\n",
    "    \"product_popularity_score\",\n",
    "    percent_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "print(f\"\\nProduct features created: {product_features.count():,} products\")\n",
    "product_features.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product features created: 3,631 products\n",
      "+----------+----------------------+--------------------+---------------------+------------------+-----------------+-----------------+------------------+-----------------------+------------------------+\n",
      "|Product_ID|product_purchase_count|product_unique_users|product_total_revenue| product_avg_price|product_min_price|product_max_price| product_std_price|product_popularity_rank|product_popularity_score|\n",
      "+----------+----------------------+--------------------+---------------------+------------------+-----------------+-----------------+------------------+-----------------------+------------------------+\n",
      "| P00265242|                  1880|                1880|             14165515| 7534.848404255319|             1720|             8907|1683.9859563927332|                      1|                     0.0|\n",
      "| P00025442|                  1615|                1615|             27995166| 17334.46811145511|             3961|            19707|2955.6096924901717|                      2|    2.754820936639118E-4|\n",
      "| P00110742|                  1612|                1612|             26722309| 16577.11476426799|             3798|            19708|3266.7937866962766|                      3|    5.509641873278236E-4|\n",
      "| P00112142|                  1562|                1562|             24216006|15503.204865556978|             3793|            19706| 3574.019614921421|                      4|    8.264462809917355E-4|\n",
      "| P00057642|                  1470|                1470|             23102780|15716.176870748299|             3890|            19708| 3470.043730193882|                      5|    0.001101928374655...|\n",
      "+----------+----------------------+--------------------+---------------------+------------------+-----------------+-----------------+------------------+-----------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering - Category-Level Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:52.008630Z",
     "start_time": "2025-11-25T21:51:50.863609Z"
    }
   },
   "source": [
    "# Category aggregations\n",
    "category_features = df_clean.groupBy(\"Product_Category_1\").agg(\n",
    "    count(\"*\").alias(\"category_purchase_count\"),\n",
    "    avg(\"Purchase\").alias(\"category_avg_price\"),\n",
    "    _sum(\"Purchase\").alias(\"category_total_revenue\")\n",
    ")\n",
    "\n",
    "print(f\"\\nCategory features: {category_features.count()} categories\")\n",
    "category_features.orderBy(col(\"category_total_revenue\").desc()).show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category features: 20 categories\n",
      "+------------------+-----------------------+------------------+----------------------+\n",
      "|Product_Category_1|category_purchase_count|category_avg_price|category_total_revenue|\n",
      "+------------------+-----------------------+------------------+----------------------+\n",
      "|                 1|                 140378|13606.218595506418|            1910013754|\n",
      "|                 5|                 150933| 6240.088178198273|             941835229|\n",
      "|                 8|                 113925| 7498.958077682686|             854318799|\n",
      "|                 6|                  20466|15838.478549789896|             324150302|\n",
      "|                 2|                  23864| 11251.93538384177|             268516186|\n",
      "|                 3|                  20213|10096.705733933608|             204084713|\n",
      "|                16|                   9828|14766.037037037036|             145120612|\n",
      "|                11|                  24287| 4685.268456375839|             113791115|\n",
      "|                10|                   5125|19675.570926829267|             100837301|\n",
      "|                15|                   6290|14780.451828298887|              92969042|\n",
      "+------------------+-----------------------+------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RFM Features (Recency, Frequency, Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:55.483915Z",
     "start_time": "2025-11-25T21:51:52.048889Z"
    }
   },
   "source": [
    "# Create RFM features\n",
    "# Note: Dataset doesn't have timestamps, so we'll create proxy RFM based on transaction ordering\n",
    "\n",
    "# Frequency: Already have user_purchase_count\n",
    "# Monetary: Already have user_total_spent\n",
    "# Recency: Use row_number as proxy for time (last purchase)\n",
    "\n",
    "window_user = Window.partitionBy(\"User_ID\").orderBy(col(\"Purchase\").desc())\n",
    "\n",
    "df_with_recency = df_clean.withColumn(\n",
    "    \"transaction_order\",\n",
    "    row_number().over(window_user)\n",
    ")\n",
    "\n",
    "# Get recency (1 = most recent for each user)\n",
    "recency_features = df_with_recency.filter(col(\"transaction_order\") == 1).select(\n",
    "    \"User_ID\",\n",
    "    col(\"Purchase\").alias(\"last_purchase_amount\")\n",
    ")\n",
    "\n",
    "# Join recency with user features to create full RFM\n",
    "rfm_features = user_features.join(\n",
    "    recency_features,\n",
    "    on=\"User_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# RFM Scoring (quintiles)\n",
    "rfm_features = rfm_features.withColumn(\n",
    "    \"rfm_frequency_score\",\n",
    "    percent_rank().over(Window.orderBy(col(\"user_purchase_count\")))\n",
    ")\n",
    "\n",
    "rfm_features = rfm_features.withColumn(\n",
    "    \"rfm_monetary_score\",\n",
    "    percent_rank().over(Window.orderBy(col(\"user_total_spent\")))\n",
    ")\n",
    "\n",
    "# Combined RFM score\n",
    "rfm_features = rfm_features.withColumn(\n",
    "    \"rfm_score\",\n",
    "    (col(\"rfm_frequency_score\") + col(\"rfm_monetary_score\")) / 2\n",
    ")\n",
    "\n",
    "print(\"\\nRFM Features:\")\n",
    "rfm_features.select(\n",
    "    \"User_ID\", \"user_purchase_count\", \"user_total_spent\",\n",
    "    \"rfm_frequency_score\", \"rfm_monetary_score\", \"rfm_score\"\n",
    ").show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RFM Features:\n",
      "+-------+-------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|User_ID|user_purchase_count|user_total_spent| rfm_frequency_score|  rfm_monetary_score|           rfm_score|\n",
      "+-------+-------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|1004464|                  9|           46681|0.003056027164685...|                 0.0|0.001528013582342...|\n",
      "|1000094|                  7|           49288|1.697792869269949...|1.697792869269949...|1.697792869269949...|\n",
      "|1003883|                  9|           49349|0.003056027164685...|3.395585738539898...|0.001697792869269949|\n",
      "|1005117|                  9|           49668|0.003056027164685...|5.093378607809847E-4|0.001782682512733...|\n",
      "|1004991|                  7|           52371|1.697792869269949...|6.791171477079797E-4|4.244482173174872...|\n",
      "|1005944|                 15|           53996| 0.06791171477079797|8.488964346349745E-4| 0.03438030560271647|\n",
      "|1002111|                  7|           54536|1.697792869269949...|0.001018675721561...|5.942275042444821E-4|\n",
      "|1003291|                  8|           55372|0.001358234295415...|0.001188455008488...|0.001273344651952...|\n",
      "|1003275|                  8|           55960|0.001358234295415...|0.001358234295415...|0.001358234295415...|\n",
      "|1004636|                 16|           57805| 0.08913412563667232|0.001528013582342...| 0.04533106960950764|\n",
      "+-------+-------------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Join All Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:58.225669Z",
     "start_time": "2025-11-25T21:51:55.529985Z"
    }
   },
   "source": [
    "# Join user features back to main dataset\n",
    "df_enriched = df_clean.join(\n",
    "    broadcast(user_features),\n",
    "    on=\"User_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join product features\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(product_features),\n",
    "    on=\"Product_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join category features\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(category_features),\n",
    "    on=\"Product_Category_1\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join RFM features\n",
    "df_enriched = df_enriched.join(\n",
    "    broadcast(rfm_features.select(\n",
    "        \"User_ID\", \"rfm_frequency_score\", \"rfm_monetary_score\", \"rfm_score\"\n",
    "    )),\n",
    "    on=\"User_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Enriched dataset: {df_enriched.count():,} rows x {len(df_enriched.columns)} columns\")\n",
    "print(\"\\nNew features added:\")\n",
    "print([c for c in df_enriched.columns if c not in df_clean.columns])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Enriched dataset: 550,068 rows x 37 columns\n",
      "\n",
      "New features added:\n",
      "['user_purchase_count', 'user_total_spent', 'user_avg_purchase', 'user_min_purchase', 'user_max_purchase', 'user_std_purchase', 'user_unique_products', 'user_unique_categories', 'user_purchase_range', 'user_avg_products_per_transaction', 'product_purchase_count', 'product_unique_users', 'product_total_revenue', 'product_avg_price', 'product_min_price', 'product_max_price', 'product_std_price', 'product_popularity_rank', 'product_popularity_score', 'category_purchase_count', 'category_avg_price', 'category_total_revenue', 'rfm_frequency_score', 'rfm_monetary_score', 'rfm_score']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:51:58.313295Z",
     "start_time": "2025-11-25T21:51:58.267917Z"
    }
   },
   "source": [
    "# Create interaction features\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"purchase_vs_user_avg_ratio\",\n",
    "    col(\"Purchase\") / col(\"user_avg_purchase\")\n",
    ")\n",
    "\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"purchase_vs_product_avg_ratio\",\n",
    "    col(\"Purchase\") / col(\"product_avg_price\")\n",
    ")\n",
    "\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"purchase_vs_category_avg_ratio\",\n",
    "    col(\"Purchase\") / col(\"category_avg_price\")\n",
    ")\n",
    "\n",
    "# Is this purchase above user's average?\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"is_above_user_avg\",\n",
    "    when(col(\"Purchase\") > col(\"user_avg_purchase\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# High value customer flag (top 20%)\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"is_high_value_customer\",\n",
    "    when(col(\"rfm_score\") >= 0.8, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Interaction features created\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Interaction features created\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:52:08.897934Z",
     "start_time": "2025-11-25T21:51:58.319436Z"
    }
   },
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [\n",
    "    \"Gender\",\n",
    "    \"Age\",\n",
    "    \"City_Category\",\n",
    "    \"Stay_In_Current_City_Years\"\n",
    "]\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\", dropLast=False)\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "encoding_pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Fit and transform\n",
    "encoding_model = encoding_pipeline.fit(df_enriched)\n",
    "df_encoded = encoding_model.transform(df_enriched)\n",
    "\n",
    "print(\"\\nâœ“ Categorical encoding completed\")\n",
    "print(f\"Total columns after encoding: {len(df_encoded.columns)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Categorical encoding completed\n",
      "Total columns after encoding: 50\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Selection for ML"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:52:08.991634Z",
     "start_time": "2025-11-25T21:52:08.943737Z"
    }
   },
   "source": [
    "# Select features for ML models\n",
    "feature_cols = [\n",
    "    # Categorical (indexed)\n",
    "    \"Gender_index\", \"Age_index\", \"City_Category_index\",\n",
    "    \"Stay_In_Current_City_Years_index\",\n",
    "    \n",
    "    # Numerical demographics\n",
    "    \"Occupation\", \"Marital_Status\",\n",
    "    \n",
    "    # Product categories\n",
    "    \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\",\n",
    "    \n",
    "    # User features\n",
    "    \"user_purchase_count\", \"user_total_spent\", \"user_avg_purchase\",\n",
    "    \"user_std_purchase\", \"user_unique_products\", \"user_unique_categories\",\n",
    "    \n",
    "    # Product features\n",
    "    \"product_purchase_count\", \"product_unique_users\", \"product_avg_price\",\n",
    "    \"product_popularity_score\",\n",
    "    \n",
    "    # Category features\n",
    "    \"category_avg_price\",\n",
    "    \n",
    "    # RFM features\n",
    "    \"rfm_frequency_score\", \"rfm_monetary_score\", \"rfm_score\",\n",
    "    \n",
    "    # Interaction features\n",
    "    \"purchase_vs_user_avg_ratio\", \"purchase_vs_product_avg_ratio\",\n",
    "    \"is_above_user_avg\", \"is_high_value_customer\"\n",
    "]\n",
    "\n",
    "# Fill any remaining nulls with 0\n",
    "df_encoded = df_encoded.fillna(0, subset=feature_cols)\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df_features = assembler.transform(df_encoded)\n",
    "\n",
    "print(f\"\\nâœ“ Feature vector assembled with {len(feature_cols)} features\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Feature vector assembled with 27 features\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:52:16.962308Z",
     "start_time": "2025-11-25T21:52:08.997533Z"
    }
   },
   "source": [
    "# Standard Scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=False,  # Sparse vectors don't support centering\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_scaled = scaler_model.transform(df_features)\n",
    "\n",
    "print(\"\\nâœ“ Features scaled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Features scaled\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Feature Set Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:52:24.621057Z",
     "start_time": "2025-11-25T21:52:17.027810Z"
    }
   },
   "source": [
    "# Select final columns for ML\n",
    "final_cols = [\n",
    "    # IDs\n",
    "    \"User_ID\", \"Product_ID\",\n",
    "    \n",
    "    # Target\n",
    "    \"Purchase\",\n",
    "    \n",
    "    # Original features\n",
    "    \"Gender\", \"Age\", \"Occupation\", \"City_Category\",\n",
    "    \"Stay_In_Current_City_Years\", \"Marital_Status\",\n",
    "    \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\",\n",
    "    \n",
    "    # Engineered features (all)\n",
    "] + [c for c in df_scaled.columns if c.startswith((\"user_\", \"product_\", \"category_\", \"rfm_\", \"is_\", \"purchase_vs\"))] + [\n",
    "    # ML-ready features\n",
    "    \"features\", \"scaled_features\"\n",
    "]\n",
    "\n",
    "df_final = df_scaled.select(*final_cols)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final dataset: {df_final.count():,} rows x {len(df_final.columns)} columns\")\n",
    "print(f\"\\nTotal features in vector: {len(feature_cols)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(f\"  - User features: {len([c for c in feature_cols if c.startswith('user_')])}\")\n",
    "print(f\"  - Product features: {len([c for c in feature_cols if c.startswith('product_')])}\")\n",
    "print(f\"  - Category features: {len([c for c in feature_cols if c.startswith('category_')])}\")\n",
    "print(f\"  - RFM features: {len([c for c in feature_cols if c.startswith('rfm_')])}\")\n",
    "print(f\"  - Interaction features: {len([c for c in feature_cols if 'ratio' in c or 'is_' in c])}\")\n",
    "print(f\"  - Demographics: {len([c for c in feature_cols if c in categorical_cols or c in ['Occupation', 'Marital_Status']])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "Final dataset: 550,068 rows x 44 columns\n",
      "\n",
      "Total features in vector: 27\n",
      "\n",
      "Feature categories:\n",
      "  - User features: 6\n",
      "  - Product features: 4\n",
      "  - Category features: 1\n",
      "  - RFM features: 3\n",
      "  - Interaction features: 4\n",
      "  - Demographics: 2\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:52:35.622784Z",
     "start_time": "2025-11-25T21:52:24.683308Z"
    }
   },
   "source": [
    "# Show sample\n",
    "df_final.select(\n",
    "    \"User_ID\", \"Product_ID\", \"Purchase\",\n",
    "    \"user_avg_purchase\", \"product_popularity_score\", \"rfm_score\"\n",
    ").show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+------------------+------------------------+------------------+\n",
      "|User_ID|Product_ID|Purchase| user_avg_purchase|product_popularity_score|         rfm_score|\n",
      "+-------+----------+--------+------------------+------------------------+------------------+\n",
      "|1000125| P00182142|   19064|14017.444444444445|    0.014600550964187328|0.4241086587436333|\n",
      "|1000146| P00088242|   12053| 9484.264705882353|      0.2793388429752066|0.9141765704584041|\n",
      "|1000163| P00270942|   12017| 6028.527675276753|    0.007988980716253443|0.8886247877758914|\n",
      "|1000170| P00111142|   19696| 12508.53488372093|    0.009366391184573003|0.4659592529711375|\n",
      "|1000438| P00254242|    8868|  8671.30745341615|     0.06666666666666667| 0.951528013582343|\n",
      "|1000541| P00023342|    5473| 8784.018018018018|     0.43526170798898073|0.7220713073005094|\n",
      "|1000588| P00116942|   16549| 9311.297709923665|      0.2862258953168044|0.9272495755517827|\n",
      "|1000620| P00127942|   11734|  7990.36170212766|     0.06253443526170799|0.6546689303904923|\n",
      "|1000646| P00145042|   19440|12161.993333333334|    0.002203856749311...|0.8421052631578947|\n",
      "|1000712| P00001042|   11957|10847.727272727272|     0.06887052341597796|0.8509337860780986|\n",
      "+-------+----------+--------+------------------+------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 14. Save to Delta Lake"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:52:49.735263Z",
     "start_time": "2025-11-25T21:52:35.694711Z"
    }
   },
   "source": "# Save to Delta Lake\ndelta_path = \"../data/processed/delta/features\"\n\ndf_final.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"City_Category\", \"Age\") \\\n    .save(delta_path)\n\nprint(f\"\\nâœ“ Data saved to Delta Lake: {delta_path}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Data saved to Delta Lake: ../data/processed/delta/features\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:53:02.086003Z",
     "start_time": "2025-11-25T21:52:49.791763Z"
    }
   },
   "source": "# Save individual feature sets for specific use cases\n\n# 1. User-level features (for segmentation)\nrfm_features.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(\"../data/processed/delta/user_features\")\n\nprint(\"âœ“ User features saved\")\n\n# 2. Product-level features (for recommendations)\nproduct_features.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(\"../data/processed/delta/product_features\")\n\nprint(\"âœ“ Product features saved\")\n\n# 3. Category features\ncategory_features.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(\"../data/processed/delta/category_features\")\n\nprint(\"âœ“ Category features saved\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ… ALL DATA SAVED TO DELTA LAKE\")\nprint(\"=\"*60)\nprint(\"\\nSaved locations:\")\nprint(\"  - Main features: data/processed/delta/features/\")\nprint(\"  - User features: data/processed/delta/user_features/\")\nprint(\"  - Product features: data/processed/delta/product_features/\")\nprint(\"  - Category features: data/processed/delta/category_features/\")\n\nprint(\"\\nðŸ“– How to load:\")\nprint(\"\"\"\n# Load from Delta Lake\ndf = spark.read.format(\"delta\").load(\"../data/processed/delta/features\")\nusers = spark.read.format(\"delta\").load(\"../data/processed/delta/user_features\")\nproducts = spark.read.format(\"delta\").load(\"../data/processed/delta/product_features\")\n\"\"\")\nprint(\"=\"*60)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ User features saved\n",
      "âœ“ Product features saved\n",
      "âœ“ Category features saved\n",
      "\n",
      "============================================================\n",
      "âœ… ALL DATA SAVED TO DELTA LAKE\n",
      "============================================================\n",
      "\n",
      "Saved locations:\n",
      "  - Main features: data/processed/delta/features/\n",
      "  - User features: data/processed/delta/user_features/\n",
      "  - Product features: data/processed/delta/product_features/\n",
      "  - Category features: data/processed/delta/category_features/\n",
      "\n",
      "ðŸ“– How to load:\n",
      "\n",
      "# Load from Delta Lake\n",
      "df = spark.read.format(\"delta\").load(\"../data/processed/delta/features\")\n",
      "users = spark.read.format(\"delta\").load(\"../data/processed/delta/user_features\")\n",
      "products = spark.read.format(\"delta\").load(\"../data/processed/delta/product_features\")\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Validation & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:53:10.362660Z",
     "start_time": "2025-11-25T21:53:02.145620Z"
    }
   },
   "source": [
    "# Feature statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select numerical features for stats\n",
    "numerical_features = [\n",
    "    \"Purchase\",\n",
    "    \"user_avg_purchase\", \"user_purchase_count\", \"user_total_spent\",\n",
    "    \"product_popularity_score\", \"product_avg_price\",\n",
    "    \"rfm_score\"\n",
    "]\n",
    "\n",
    "df_final.select(numerical_features).describe().show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE STATISTICS\n",
      "============================================================\n",
      "+-------+-----------------+------------------+-------------------+------------------+------------------------+------------------+--------------------+\n",
      "|summary|         Purchase| user_avg_purchase|user_purchase_count|  user_total_spent|product_popularity_score| product_avg_price|           rfm_score|\n",
      "+-------+-----------------+------------------+-------------------+------------------+------------------------+------------------+--------------------+\n",
      "|  count|           550068|            550068|             550068|            550068|                  550068|            550068|              550068|\n",
      "|   mean|9263.968712959126| 9263.968712959091| 216.40340103405399|1925314.2293225564|     0.18545974861790282| 9263.968712959126|  0.7600518627960817|\n",
      "| stddev|5023.065393820586|1619.5105670471598| 175.03115005145187|1489900.5977689005|      0.1842330766840676| 4267.891837442049| 0.23262711582228426|\n",
      "|    min|               12| 2318.733333333333|                  6|             46681|                     0.0| 36.67515923566879|1.697792869269949...|\n",
      "|    max|            23961|18577.893617021276|               1026|          10536909|      0.9606060606060606|21256.505494505494|  0.9999151103565365|\n",
      "+-------+-----------------+------------------+-------------------+------------------+------------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:53:17.505122Z",
     "start_time": "2025-11-25T21:53:10.420726Z"
    }
   },
   "source": [
    "# Check for nulls in final dataset\n",
    "null_counts = df_final.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_final.columns[:20]  # First 20 cols\n",
    "])\n",
    "\n",
    "print(\"\\nNull counts in final dataset (first 20 columns):\")\n",
    "null_counts.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null counts in final dataset (first 20 columns):\n",
      "+-------+----------+--------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+-------------------+----------------+-----------------+-----------------+-----------------+-----------------+--------------------+----------------------+\n",
      "|User_ID|Product_ID|Purchase|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|user_purchase_count|user_total_spent|user_avg_purchase|user_min_purchase|user_max_purchase|user_std_purchase|user_unique_products|user_unique_categories|\n",
      "+-------+----------+--------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+-------------------+----------------+-----------------+-----------------+-----------------+-----------------+--------------------+----------------------+\n",
      "|      0|         0|       0|     0|  0|         0|            0|                         0|             0|                 0|                 0|                 0|                  0|               0|                0|                0|                0|                0|                   0|                     0|\n",
      "+-------+----------+--------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+-------------------+----------------+-----------------+-----------------+-----------------+-----------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:53:30.248317Z",
     "start_time": "2025-11-25T21:53:17.569742Z"
    }
   },
   "source": [
    "# Feature correlation with target (Purchase)\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = Correlation.corr(df_final, \"scaled_features\").head()\n",
    "\n",
    "print(\"\\nâœ“ Correlation matrix computed\")\n",
    "print(\"(Can be used for feature selection)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Correlation matrix computed\n",
      "(Can be used for feature selection)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:53:30.310078Z",
     "start_time": "2025-11-25T21:53:30.306386Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… FEATURE ENGINEERING PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š Created Features:\")\n",
    "print(\"  âœ“ User-level aggregations (purchase patterns)\")\n",
    "print(\"  âœ“ Product-level aggregations (popularity, pricing)\")\n",
    "print(\"  âœ“ Category-level aggregations\")\n",
    "print(\"  âœ“ RFM features (Frequency, Monetary)\")\n",
    "print(\"  âœ“ Interaction features (ratios, flags)\")\n",
    "print(\"  âœ“ Categorical encoding (StringIndexer + OneHot)\")\n",
    "print(\"  âœ“ Feature scaling (StandardScaler)\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved Artifacts:\")\n",
    "print(\"  âœ“ Main features dataset â†’ data/processed/delta/features/\")\n",
    "print(\"  âœ“ User features â†’ data/processed/delta/user_features/\")\n",
    "print(\"  âœ“ Product features â†’ data/processed/delta/product_features/\")\n",
    "print(\"  âœ“ Encoding pipeline â†’ models/encoding_pipeline/\")\n",
    "print(\"  âœ“ Scaler model â†’ models/scaler_model/\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"  1. Build ETL pipeline (automation)\")\n",
    "print(\"  2. Train ML models (regression, clustering, recommendation)\")\n",
    "print(\"  3. Model evaluation and tuning\")\n",
    "print(\"  4. Setup Kafka + Streaming pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… FEATURE ENGINEERING PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Created Features:\n",
      "  âœ“ User-level aggregations (purchase patterns)\n",
      "  âœ“ Product-level aggregations (popularity, pricing)\n",
      "  âœ“ Category-level aggregations\n",
      "  âœ“ RFM features (Frequency, Monetary)\n",
      "  âœ“ Interaction features (ratios, flags)\n",
      "  âœ“ Categorical encoding (StringIndexer + OneHot)\n",
      "  âœ“ Feature scaling (StandardScaler)\n",
      "\n",
      "ðŸ’¾ Saved Artifacts:\n",
      "  âœ“ Main features dataset â†’ data/processed/delta/features/\n",
      "  âœ“ User features â†’ data/processed/delta/user_features/\n",
      "  âœ“ Product features â†’ data/processed/delta/product_features/\n",
      "  âœ“ Encoding pipeline â†’ models/encoding_pipeline/\n",
      "  âœ“ Scaler model â†’ models/scaler_model/\n",
      "\n",
      "ðŸŽ¯ Next Steps:\n",
      "  1. Build ETL pipeline (automation)\n",
      "  2. Train ML models (regression, clustering, recommendation)\n",
      "  3. Model evaluation and tuning\n",
      "  4. Setup Kafka + Streaming pipeline\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:53:30.317317Z",
     "start_time": "2025-11-25T21:53:30.315585Z"
    }
   },
   "source": [
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "# print(\"\\nâœ“ Spark session stopped\")"
   ],
   "outputs": [],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
