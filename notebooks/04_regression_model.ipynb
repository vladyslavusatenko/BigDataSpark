{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model - Purchase Prediction\n",
    "\n",
    "**Cel:** Zbudowaƒá model predykcyjny do przewidywania warto≈õci zakupu (Purchase)\n",
    "\n",
    "**Modele do przetestowania:**\n",
    "1. Linear Regression (baseline)\n",
    "2. Random Forest Regressor\n",
    "3. Gradient Boosted Trees (GBT)\n",
    "\n",
    "**Metryki ewaluacji:**\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- R¬≤ (R-squared)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, stddev, min as _min, max as _max\n",
    "from config.spark_config import SparkConfig\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkConfig.get_spark_session(\"BlackFriday-RegressionModel\")\n",
    "print(\"‚úì Spark ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Features\n",
    "\n",
    "**Note:** Try Delta Lake first. If it fails, fall back to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading from Delta Lake\n",
    "try:\n",
    "    print(\"Loading from Delta Lake...\")\n",
    "    df_features = spark.read.format(\"delta\").load(\"../data/processed/delta/features\")\n",
    "    print(\"‚úì Loaded from Delta Lake\")\nexcept Exception as e:\n",
    "    print(f\"Delta Lake failed: {e}\")\n",
    "    print(\"\\nTrying Parquet format...\")\n",
    "    df_features = spark.read.parquet(\"../data/processed/parquet/features\")\n",
    "    print(\"‚úì Loaded from Parquet\")\n",
    "\n",
    "print(f\"\\nDataset: {df_features.count():,} rows x {len(df_features.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only required columns for ML\n",
    "ml_data = df_features.select(\n",
    "    \"Purchase\",           # Target variable\n",
    "    \"scaled_features\"     # Feature vector (scaled)\n",
    ").withColumnRenamed(\"scaled_features\", \"features\")\n",
    "\n",
    "# Check for nulls\n",
    "null_count = ml_data.filter(col(\"Purchase\").isNull() | col(\"features\").isNull()).count()\n",
    "print(f\"Rows with nulls: {null_count}\")\n",
    "\n",
    "if null_count > 0:\n",
    "    print(\"Dropping rows with nulls...\")\n",
    "    ml_data = ml_data.dropna()\n",
    "    print(f\"‚úì Clean dataset: {ml_data.count():,} rows\")\n",
    "\n",
    "# Cache for performance\n",
    "ml_data.cache()\n",
    "print(\"\\n‚úì Data cached for faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "purchase_stats = ml_data.select(\n",
    "    count(\"Purchase\").alias(\"count\"),\n",
    "    avg(\"Purchase\").alias(\"mean\"),\n",
    "    stddev(\"Purchase\").alias(\"std\"),\n",
    "    _min(\"Purchase\").alias(\"min\"),\n",
    "    _max(\"Purchase\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TARGET VARIABLE STATISTICS (Purchase)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Count:   {purchase_stats['count']:,}\")\n",
    "print(f\"Mean:    ${purchase_stats['mean']:,.2f}\")\n",
    "print(f\"Std:     ${purchase_stats['std']:,.2f}\")\n",
    "print(f\"Min:     ${purchase_stats['min']:,.2f}\")\n",
    "print(f\"Max:     ${purchase_stats['max']:,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "purchase_sample = ml_data.select(\"Purchase\").sample(False, 0.01, seed=42).toPandas()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(purchase_sample['Purchase'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "ax1.set_title('Purchase Amount Distribution (1% sample)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Purchase ($)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(purchase_stats['mean'], color='red', linestyle='--', label=f\"Mean: ${purchase_stats['mean']:,.0f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(purchase_sample['Purchase'], vert=True)\n",
    "ax2.set_title('Purchase Amount Box Plot', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Purchase ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache both sets\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set:   {train_data.count():,} rows ({train_data.count() / ml_data.count() * 100:.1f}%)\")\n",
    "print(f\"Test set:       {test_data.count():,} rows ({test_data.count() / ml_data.count() * 100:.1f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "### 6.1 Baseline: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING: LINEAR REGRESSION (Baseline)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Purchase\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.0  # L2 regularization\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training...\")\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"‚úì Training complete!\")\n",
    "\n",
    "# Training metrics\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  RMSE: ${lr_model.summary.rootMeanSquaredError:,.2f}\")\n",
    "print(f\"  MAE:  ${lr_model.summary.meanAbsoluteError:,.2f}\")\n",
    "print(f\"  R¬≤:   {lr_model.summary.r2:.4f}\")\n",
    "print(f\"  Iterations: {lr_model.summary.totalIterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# RMSE\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"Purchase\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "\n",
    "# MAE\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"Purchase\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "\n",
    "# R¬≤\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"Purchase\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINEAR REGRESSION - TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE:  ${lr_rmse:,.2f}\")\n",
    "print(f\"MAE:   ${lr_mae:,.2f}\")\n",
    "print(f\"R¬≤:    {lr_r2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Purchase\",\n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training (this may take a few minutes)...\")\n",
    "rf_model = rf.fit(train_data)\n",
    "print(\"‚úì Training complete!\")\n",
    "print(f\"\\nTrees trained: {rf_model.getNumTrees}\")\n",
    "print(f\"Max depth: {rf_model.getOrDefault('maxDepth')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST - TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE:  ${rf_rmse:,.2f}\")\n",
    "print(f\"MAE:   ${rf_mae:,.2f}\")\n",
    "print(f\"R¬≤:    {rf_r2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Gradient Boosted Trees (GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING: GRADIENT BOOSTED TREES (GBT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Purchase\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training (this may take several minutes)...\")\n",
    "gbt_model = gbt.fit(train_data)\n",
    "print(\"‚úì Training complete!\")\n",
    "print(f\"\\nTrees trained: {gbt_model.getNumTrees}\")\n",
    "print(f\"Max depth: {gbt_model.getOrDefault('maxDepth')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
    "gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
    "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRADIENT BOOSTED TREES - TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE:  ${gbt_rmse:,.2f}\")\n",
    "print(f\"MAE:   ${gbt_mae:,.2f}\")\n",
    "print(f\"R¬≤:    {gbt_r2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'Gradient Boosted Trees'],\n",
    "    'RMSE': [lr_rmse, rf_rmse, gbt_rmse],\n",
    "    'MAE': [lr_mae, rf_mae, gbt_mae],\n",
    "    'R¬≤': [lr_r2, rf_r2, gbt_r2]\n",
    "})\n",
    "\n",
    "# Sort by RMSE (lower is better)\n",
    "results = results.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "best_rmse = results.iloc[0]['RMSE']\n",
    "best_r2 = results.iloc[0]['R¬≤']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   RMSE: ${best_rmse:,.2f}\")\n",
    "print(f\"   R¬≤: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(results['Model'], results['RMSE'], color=['skyblue', 'coral', 'lightgreen'])\n",
    "axes[0].set_title('RMSE Comparison (lower is better)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('RMSE ($)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(results['Model'], results['MAE'], color=['skyblue', 'coral', 'lightgreen'])\n",
    "axes[1].set_title('MAE Comparison (lower is better)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('MAE ($)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R¬≤ comparison\n",
    "axes[2].bar(results['Model'], results['R¬≤'], color=['skyblue', 'coral', 'lightgreen'])\n",
    "axes[2].set_title('R¬≤ Comparison (higher is better)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('R¬≤')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "**Note:** Only tree-based models (RF, GBT) provide feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature names (from original feature engineering)\n",
    "# Note: This is a simplified version. In production, you'd save feature names with the model.\n",
    "feature_names = [\n",
    "    'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status',\n",
    "    'Product_Category_1', 'Product_Category_2', 'Product_Category_3',\n",
    "    'user_purchase_count', 'user_total_spent', 'user_avg_purchase', 'user_std_purchase',\n",
    "    'user_unique_products', 'product_purchase_count', 'product_unique_customers',\n",
    "    'product_avg_price', 'product_total_revenue', 'product_popularity_score',\n",
    "    'category_purchase_count', 'category_avg_price', 'rfm_frequency_score',\n",
    "    'rfm_monetary_score', 'rfm_score', 'purchase_vs_user_avg_ratio',\n",
    "    'purchase_vs_product_avg_ratio', 'is_high_value_customer'\n",
    "]\n",
    "\n",
    "# Get importance scores\n",
    "rf_importance = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names[:len(rf_importance)],  # Match length\n",
    "    'Importance': rf_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display top 15\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions from best model (use RF as example)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION ANALYSIS (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample predictions\n",
    "sample_predictions = rf_predictions.select(\"Purchase\", \"prediction\").sample(False, 0.001, seed=42).toPandas()\n",
    "\n",
    "# Calculate residuals\n",
    "sample_predictions['residual'] = sample_predictions['Purchase'] - sample_predictions['prediction']\n",
    "sample_predictions['abs_residual'] = np.abs(sample_predictions['residual'])\n",
    "sample_predictions['pct_error'] = (sample_predictions['abs_residual'] / sample_predictions['Purchase']) * 100\n",
    "\n",
    "print(f\"\\nSample size: {len(sample_predictions):,}\")\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean Absolute Error: ${sample_predictions['abs_residual'].mean():,.2f}\")\n",
    "print(f\"  Median Absolute Error: ${sample_predictions['abs_residual'].median():,.2f}\")\n",
    "print(f\"  Mean Percentage Error: {sample_predictions['pct_error'].mean():.2f}%\")\n",
    "print(f\"  Median Percentage Error: {sample_predictions['pct_error'].median():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "axes[0].scatter(sample_predictions['Purchase'], sample_predictions['prediction'], \n",
    "                alpha=0.5, s=20, color='skyblue', edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([sample_predictions['Purchase'].min(), sample_predictions['Purchase'].max()],\n",
    "             [sample_predictions['Purchase'].min(), sample_predictions['Purchase'].max()],\n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Purchase ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Purchase ($)', fontsize=12)\n",
    "axes[0].set_title('Actual vs Predicted Values', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "axes[1].scatter(sample_predictions['prediction'], sample_predictions['residual'],\n",
    "                alpha=0.5, s=20, color='coral', edgecolors='black', linewidth=0.5)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Purchase ($)', fontsize=12)\n",
    "axes[1].set_ylabel('Residual ($)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(sample_predictions['residual'], bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "plt.axvline(x=0, color='red', linestyle='--', lw=2, label='Zero Error')\n",
    "plt.xlabel('Residual ($)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Best Model\n",
    "\n",
    "**Note:** On Windows, save as pickle to avoid Hadoop issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save model metadata (not the full model, just configuration and metrics)\n",
    "model_metadata = {\n",
    "    'model_type': 'RandomForestRegressor',\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': {\n",
    "        'rmse': rf_rmse,\n",
    "        'mae': rf_mae,\n",
    "        'r2': rf_r2\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'numTrees': rf_model.getNumTrees,\n",
    "        'maxDepth': rf_model.getOrDefault('maxDepth'),\n",
    "        'minInstancesPerNode': rf_model.getOrDefault('minInstancesPerNode')\n",
    "    },\n",
    "    'feature_importance': importance_df.to_dict('records'),\n",
    "    'training_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(f\"{models_dir}/regression_model_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_metadata, f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model metadata saved to: {models_dir}/regression_model_metadata.pkl\")\n",
    "print(\"\\nNote: Full Spark model can be saved with:\")\n",
    "print(\"  rf_model.save('../models/rf_regression_model')\")\n",
    "print(\"  (May require proper Hadoop setup on Windows)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Business Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Model Performance:\")\n",
    "print(f\"  - Best Model: {best_model_name}\")\n",
    "print(f\"  - Prediction Accuracy (R¬≤): {best_r2:.2%}\")\n",
    "print(f\"  - Average Error (MAE): ${rf_mae:,.2f}\")\n",
    "print(f\"  - Typical prediction is within ¬±${rf_mae:,.2f} of actual purchase\")\n",
    "\n",
    "print(\"\\nüìä Key Insights from Feature Importance:\")\n",
    "print(\"  Top 3 factors influencing purchase amount:\")\n",
    "for i, row in importance_df.head(3).iterrows():\n",
    "    print(f\"    {i+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Business Applications:\")\n",
    "print(\"  1. Revenue Forecasting: Predict expected revenue for upcoming campaigns\")\n",
    "print(\"  2. Customer Targeting: Identify high-value purchase opportunities\")\n",
    "print(\"  3. Inventory Planning: Forecast demand by product category\")\n",
    "print(\"  4. Personalized Marketing: Tailor offers based on predicted purchase value\")\n",
    "print(\"  5. Budget Allocation: Optimize ad spend by predicted customer value\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "print(\"  1. Focus marketing on customers with high RFM scores\")\n",
    "print(\"  2. Optimize product mix based on category importance\")\n",
    "print(\"  3. Implement dynamic pricing using purchase predictions\")\n",
    "print(\"  4. Create targeted campaigns for predicted high-value transactions\")\n",
    "print(\"  5. Use model to score leads and prioritize sales efforts\")\n",
    "\n",
    "print(\"\\nüîç Model Limitations:\")\n",
    "print(f\"  - R¬≤ of {best_r2:.2%} means {(1-best_r2):.2%} of variance is unexplained\")\n",
    "print(\"  - Consider additional features: time/seasonality, promotions, external factors\")\n",
    "print(\"  - May not generalize to completely new products or customers\")\n",
    "print(\"  - Requires periodic retraining as customer behavior evolves\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ REGRESSION MODEL COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìà Models Trained:\")\n",
    "print(\"  ‚úì Linear Regression (Baseline)\")\n",
    "print(\"  ‚úì Random Forest Regressor\")\n",
    "print(\"  ‚úì Gradient Boosted Trees\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"  - RMSE: ${best_rmse:,.2f}\")\n",
    "print(f\"  - MAE: ${rf_mae:,.2f}\")\n",
    "print(f\"  - R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "print(\"\\nüíæ Saved:\")\n",
    "print(\"  ‚úì Model metadata and metrics\")\n",
    "print(\"  ‚úì Feature importance scores\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  1. Build Clustering Model (Customer Segmentation)\")\n",
    "print(\"  2. Build Recommendation System (ALS)\")\n",
    "print(\"  3. Deploy model to production\")\n",
    "print(\"  4. Setup monitoring and retraining pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Stop Spark\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
